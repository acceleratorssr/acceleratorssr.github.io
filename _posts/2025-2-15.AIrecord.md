---
layout: post
title: 学习过程的随笔、记录
tags: AI
excerpt: 记录积累
---

- 归一化：将数据调整到一个标准范围内的过程，用于提高模型的收敛速度和性能；
- 核函数：SVM 等核方法中，核函数用于将输入数据映射到高维空间，以便在该空间中更容易找到数据的分离超平面；常用核函数（线性核、多项式核、高斯径向基函数（RBF）核等）

**背景**：
&emsp;&emsp;原始k8s基于物理CPU调度，物理核对性能与CPU型号、HT、Turbo 等强相关，导致相同CPU核数时，性能会不一样；

&emsp;&emsp;故需归一化解决：算力的一致性；
> HT：超线程技术，即一个物理core视为 两个逻辑core;
> 
> Turbo:睿频加速，运行cpu自动提高运行频率；

#### 预处理
**主要用于清洗数据、格式化数据**；
- 数据**清洗**：去除或修正缺失值、重复值、异常值或噪声；
- 数据**标准化和归一化**：将数据缩放到一个特定的范围或标准化为特定均值和方差分布，加快训练速度并提高模型稳定性；
- 数据**转换**：将数据转换为模型可接收的格式，如文本转为向量(词袋模型、词嵌入embedding)、图像转为特定的尺寸和通道数；
- 特征工程：挑选和转换数据中特定的特征，或者通过算法生成新特征，提高模型的表现力和准确性；
- 数据集划分：将数据集划分为训练集、验证集和测试集，用于模型训练+评估；

#### 后处理
&emsp;&emsp;对模型生成预测或输出结果后对结果进行的处理，目的是提高结果的可解释性、可用性，根据具体应用需求对结果进行调整；

&emsp;&emsp;如：

&emsp;&emsp;&emsp;&emsp;结果解码：如分类模型输出后解码为具体标签；

&emsp;&emsp;&emsp;&emsp;阈值设定：分类任务中，超阈值的结果转换为具体的类别；

&emsp;&emsp;&emsp;&emsp;结果过滤和清理：去除不合理结果；

&emsp;&emsp;&emsp;&emsp;集合方法：对多个模型的输出进行聚合，以提高总体的准确性和稳健性，如投票法，加权平均法等（类似随机森林）；

#### 模型文件
&emsp;&emsp;模型架构：模型层数、每一层的神经元书面、激活函数类型等信息；

&emsp;&emsp;权重和偏置；

&emsp;&emsp;配置文件，优化器的设置、学习率、训练时长等；

#### 词袋模型(BoW)：
&emsp;&emsp;将文本表示为词汇表中词的集合，而不考虑词的顺序和语法；

特点：

&emsp;&emsp;1、词汇表创建：从语料库中提取唯一词汇，构建一个固定大小的词汇表；

&emsp;&emsp;2、向量化文本：文档被向量化后，向量中每个元素对应词汇表中一个词在文档中出现的频次(TF, Term Frequency)

&emsp;&emsp;&emsp;&emsp;如：词汇表["a", "b", "c"]，文档是"a c a”，那么此时文档向量化后的结果是 [2, 0, 1]；

&emsp;&emsp;3、TF-IDF：增强版的词袋模型，考虑术语频率的基础上，还会考虑逆文档频率（IDF，Inverse Document Fequecy)，即词在整个语料库中出现的频率；（结论：降低常见词的权重，相对提高那些特定文档中频繁但在整个语料库中少见的词的权重）

&emsp;&emsp;&emsp;&emsp;`TF-IDF(t, d)= TF(t, d) * IDF(t)`

&emsp;&emsp;&emsp;&emsp;`IDF(T) = log(N / DF(t))`

&emsp;&emsp;&emsp;&emsp;`N` 为文档总数，`DF(t)` 是含有词 t 的文档数量；

&emsp;&emsp;&emsp;&emsp;`TF(t, d) = 词 t 在文档 d 中出现的次数 / 文档 d 中所有词的总数`

#### 词嵌入(word embeddings)

&emsp;&emsp;使用密集向量表示词语的方法，目标是捕获词语之间的语义联系，常见的词嵌入模型包括word2Vec、GloVe、FastText等；

特点：

&emsp;&emsp;1、低维稠密向量：

&emsp;&emsp;&emsp;&emsp;每个词被表示为一个低维的稠密向量，几十到几百；

&emsp;&emsp;&emsp;&emsp;通过训练神经网络学习得到，可捕获语义上的相似性，相似的词在向量空间中更加接近；

&emsp;&emsp;2、上下文信息：

&emsp;&emsp;训练过程中会考虑上下文信息，可捕获词语的多种语义；

#### 近似最近邻检索
ANNS (Approximate Nearest NeithRRla Search)

核心思想：不再局限于只返回最精确的结果项，仅搜索可能是近邻的数据项，精度换速度；

ANNS 向量索引分为：
- 基于树的索引：KD树：
&emsp;&emsp;&emsp;&emsp;K-dimension tree，对数据点在k维空间(如三维x，y,z)中划分的一种数据结构；

&emsp;&emsp;&emsp;&emsp;本质上就是一种平衡二叉树，在被划分出的特定几个部分进行相关检索；
1. 特征点数据 features；
2. 展开 kd-tree；
3. 选择最大方差维数 ki，Assign Partition Key
4. 选取 ki 维中值 kv 作为阈值 Median Select
5. 分割数据 Partition Feature
6. 左 / 右子树特征点数据 features -> 展开左 / 右子树；

&emsp;&emsp;当查询点的邻域与分割超平面两侧空间交割时，需要查找另一侧子空间，导致检索过程复杂，效率下降；

&emsp;&emsp;KD 树对于低维度最近邻搜索较优，当 K >= 10，搜索效率明显降低；

#### 基于图的索引
HNSW，可见[向量检索 & 多模态向量检索引擎](https://acceleratorssr.github.io/2025/02/01/vector.html)备注部分；

#### 基于哈希的索引
LSH，可见[向量检索 & 多模态向量检索引擎](https://acceleratorssr.github.io/2025/02/01/vector.html)备注部分；

#### 基于量化的索引

#### 马尔可夫决策过程(Markov Decision Process, MDP)
&emsp;&emsp;核心在于其依赖状态的决策过程，并假设未来只依赖于当前状态和当前决策（马尔可夫性质）；

组成要素：

1、状态集(S)：描述环境在某一时刻的情况的集合；

2、动作集(A)：在某一时刻，智能体可采取的所有可能动作的集合；

3、状态转移概率，表示为（状态 s 采取动作 a 后转移到状态 s' 的概率）：P(s'|s,a)

4、奖励函数：定义某一时刻下采取某个动作后获得的即时奖励，表示为 R(s, a) 或 R(s, a, s');

5、折扣因子：0~1，权衡当前奖励与未来奖励的相对重要性，接近1代表未来奖励更重要，反之当前奖励高；

#### 强化学习
reinforcement learning，RL

函数：回归（函数输出是一个值）、分类（函数输出是一个类）

生成式学习：

找出函数的三步：**设定范围**、**设定标准**、**达成目标**；