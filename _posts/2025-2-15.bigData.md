---
layout: post
title: 大数据相关 常见流程
tags: 业务开发积累
excerpt: ETL、离线批处理，随笔
---

#### ETL
&emsp;&emsp;**提取**(Extract)：从各种数据源中提取原始数据，数据可能是异构的；

&emsp;&emsp;**转换**(Transform)：对提取的数据进行清洗、处理和转换；

&emsp;&emsp;**加载**(Load)：将清洗后的数据加载到数仓等存储平台中，供后续分析使用；

完整的离线大数据分析：MySQL -> HDFS -> Hive表 -> SQL分析

即拉取数据库数据，存到分布式文件系统中，再导入数仓；

处理数据时，可能选择使用 mapreduce或spark（可持久化中间数据到内存）执行计算，他们都可通过YARN申请资源并执行计算任务；

YARN作为资源管理系统，负责协调不同的应用(mapreduce、spark)之间的资源分配，在YARN中，集群资源被分为不同的容器，每个任务会被分配到一个容器上执行；

hadoop 提供 存储(HDFS)、资源管理(YARN)、计算框架(MP、Spark、Fink、Hive、Tez)

#### 列式存储

&emsp;&emsp;核心在于 Parquet 格式，数据按列存储、支持多种压缩算法（列式存储，故相同类型数据被存储在一起，压缩效率更高)、支持多种复杂的数据类型，包括嵌套数据结构（列表、映射等）、数据的 schema 信息；

&emsp;&emsp;行式存储适合事务性操作，每一行数据都保存在连续的内存或磁盘空间中；

&emsp;&emsp;列式存储适合聚合、过滤等分析查询的场景（只需要查询相关列），且数据的压缩效率高（列内有很多重复，行基本没有行间的重复）；

分区字段：时间；

**hive+spark**:

&emsp;&emsp;hive 是一个 sql查询接口层；

&emsp;&emsp;用户在 hive提交SQL查询；

&emsp;&emsp;hive 解析并优化 sql(如选择合适的执行引擎)；

&emsp;&emsp;hive 生成执行计划(DAG)，包括数据读取、分区、聚合等操作；

**spark**:

&emsp;&emsp;通过DAG分析查询中所有任务和依赖关系，表示数据如何从一个操作传递到另一个操作；

&emsp;&emsp;懒加载机制：spark 先构建这个执行计划，当用户触发行为操作，collect()、show()时，Spark才开始计算；

&emsp;&emsp;suffle：如遇到 group by 时，spark 将数据根据 productID 分组，将数据分布到不同节点上，执行聚合计算；