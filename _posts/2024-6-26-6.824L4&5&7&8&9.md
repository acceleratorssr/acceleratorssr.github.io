---
layout: post
title: 2021L4、5、7、8、9
tags: MIT6.824
excerpt: 2021 L4、5、7、8、9
---

# 2021 L4
<h2>主备机制</h2>
<p>两种方法：状态转移复制，复制状态机；</p>
<h2>一般的错误：</h2>
<ul>
    <li>主从复制可解决：fail-stop failures，宕机</li>
    <li>不可解决：logic bugs，软件/配置出错，或者是恶意错误，即攻击；</li>
</ul>
<h2>挑战：</h2>
<ul>
    <li>如何确定主节点真的挂了；（在分布式环境中，无法区分网络分区和机器故障）</li>
    <li>如何同步主从一致 -> 必须保证顺序应用变更 &&  解决非决定论（non-determinism）；</li>
    <li>故障转移</li>
</ul>
<p>&emsp;&emsp;状态转移复制：主节点每处理完一个请求，响应前先同步检查点（checkPoint）给从节点</p>
<p>&emsp;&emsp;复制状态机（RSM）：主节点先同步操作（operation）给从节点，让从节点执行操作后响应，自己在进行操作后响应客户端；</p>
<h2>VM-FT:</h2>
<p>&emsp;&emsp;主备断开后（网络原因，双方都没挂），都请求将存储服务器的test-and-set状态标志位 置1（原子操作），第一个到达的请求正常置1，第二个到达的一端会发现该标志位已经是1了，则会终止自己；第一个成功置1的机器会变为主机，并且请求复制新的备机出来（VMFT中的修复方案由人工执行），当备机生成后，test-and-set状态标志位会被复位为0（注意，第一次的主机如果没拿到to live状态，则会被及时回收，不会在标志位复位后才请求置1）；</p>
<p>&emsp;&emsp;将中断和有关数据和信息写入日志并在日志channel发送给备机，即保存在备机的缓存中，让中断可以被顺序执行，执行中断时，cpu将控制权交给OS（即虚拟机监视器）；</p>
<p>&emsp;&emsp;FT的Boot，日志channel（位于FT层）传送非确定性的指令，此时控制器会交给FT（trap），如磁盘io等，之后将指令的结果传给备机，确保备机使用相同的值（备机因此会落后）；是因为备机的FT不会执行该指令，所以需要直接获取寄存器哪里被修改（结果）并直接应用；（主机完成操作前，备机不会执行下一个日志）</p>
<p>&emsp;&emsp;主机响应client前，需要确保日志被送到备机，虽然备机不一定会立刻执行，但是要确保它拥有该日志，如果主机挂了，则备机会在一定时间内执行完剩下的日志赶上主机后，才对外提供服务；</p>
<p>&emsp;&emsp;客户端向主机发送请求后，会有FT将请求传给主机，并且通过日志channel传给备机，当主机完成后，将响应先发到FT，FT会等待备机返回接收到日志的响应后，才将主机的响应正常交回给客户端；</p>
<p>&emsp;&emsp;不应该在指令层使用复制状态机，而是在应用层应用；</p>

# 2021 L5

一些raft的机制的解说；
<p><img src="https://acceleratorssr.github.io/image/f8.png" alt="错误提交"></p>

  a. follower落后同一任期的日志；
  b. follower落后之前任期的日志；
  c. follower的相同任期的日志index新于当前leader的日志（相比于leader新的日志一定没有被提交，即可被覆盖）；
  d. follower多出leader之后任期的日志（同样新的日志没提交可被覆盖）；
  e. follower落后之前任期的日志，但是之前任期的日志有未提交的日志状态；
  f. follower拥有leader没有的任期内的日志（同样可被覆盖）；

&emsp;&emsp;首先为了保证一致性，日志需要达成大多数才可被提交并持久化，且candidate想要胜选就需要获得大多数的投票；而对于日志已经被提交的节点集合和需要投票的节点集合一定有交集，故candidate一定属于日志已经被提交的节点集合才可能胜选；

# 2021 L7:

&emsp;&emsp;另一种优化日志回退：follower多返回冲突的term及该term的第一个日志的index值，leader可发送对应这个index上的正确日志及以后的多个新日志给follower（一次性发送多个）；

# 2021 L8:

老师使用的：间隔50ms检查timeout，超时是 1s 间隔 0~300ms（1+0 or 1+0.3）；
<pre><code>
type Raft struct {
	mu        sync.Mutex   //Lock to protect shared access to this peer's stat
	peers     []*ClientEnd //RPC end points of all peers
	persister *Persister   //Object to hold this peer's persisted state
	me        int          //this peer's index into peers[]
	dead      int32        //set by Kill()
	applyCh   chan ApplyMsg
	applyCond *sync.Cond
	state        Tstate
	electionTime time.Time

	//Look at the paper's Figure 2 for a description of what
	//state a Raft server must maintain.

	//Persistent state
	currentTerm int
	votedFor    int
	log         LogEntries

	//Volatile state
	commitIndex int
	lastApplied int

	//Leader state
	nextIndex  []int
	matchIndex []int

	//Snapshot state
	snapshot      []byte
	snapshotIndex int
	snapshotTerm  int

	//Temporary location to give the service snapshot to the
	//apply thread.
	waitingSnapshot []byte
	waitingIndex    int //lastIncludedIndex
	waitingTerm     int //lastIncludedTerm
}
</code></pre>

编码习惯：如果函数假设调用者持有锁，则在函数名末尾**加L**；

rpc请求响应后才加锁处理对应的数据。

# 2021 L9

**ZK没有提供强一致性**

### Key Idea
- 写操作都是异步的，客户端可以一次向ZooKeeper提交多个操作，会排队执行，leader对于批量的操作仅会写入一次磁盘。
- 运行读操作在任意服务器执行（故无法保证强一致性）。

### 线性一致性
- 可以构建所有操作的顺序。
- 顺序与真实时间匹配。
- 读操作返回最后写入的值。

### ZK的优化
1. 提供线性化写入。
2. 所有操作按照FIFO的顺序执行。
    - 读取操作会观察到同一客户端的最后一次写入（不一定能观察到其他客户端的最新写入）。
    - 读取时可观察到日志的某些前缀，可能读取到旧值，但不会读到相对于当前客户端的旧值，换句话说不会回到过去（即如果客户端读x获取前缀1，下一次读的有效结果前缀必须大于等于1）。
  - 每次写入（写入最后一定会回到leader）后会有对应的zxid，如果读取到一个没有对应zxid的节点时，不会立刻返回失败，而是等待节点更新到相同的zxid。（ZK也会负责处理负载均衡）
  - 节点使用exists获取到另一个节点的值存在的同时，注册对应的watch，后续如果值发生更改后，节点会收到通知（通知是在另一个节点发生下一个write时发送的），节点就可以选择重新从exists开始执行。

### Znode API
- **Znode tree**：
    - 挂载机器节点（ip或者dns），有版本号。
    - 常规节点，复制状态机。
    - 临时节点：当session离开后，或者没收到心跳，自动删除。
    - 顺序节点，在特定的znode下一个接一个排序，按照序列号排序。

### Test and Set
将1 <code>set</code>入内存，并原子返回旧值；