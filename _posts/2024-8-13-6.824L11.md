---
layout: post
title: 2021L11
tags: MIT6.824
excerpt: zk、链式赋值；
---

2021 L11
zk的分布式锁实现，简单（基本不用）版本，循环创建锁文件，创建成功即为获得锁，使用break退出循环，创建失败，则使用exists判断锁文件，目的对锁文件添加watch，当成功创建锁文件的进程释放锁，即调用delete锁文件后，其他进程被唤醒去抢夺锁，以此类推；
存在的问题，羊群效应，因为每次都有大量的节点抢夺一把锁，最后只有一个节点成功，会给zk服务器带来很大的压力；
ZK分布式锁正常的实现：见/分布式/分布式锁

zk锁：
可以用于客户端选举；
也可以用于软锁，即客户端如果不宕机就正常执行一次，但是宕机了，会导致操作会被执行两次，如mapreduce的map阶段，也是被允许的操作；

状态转移和复制状态机

两种构建复制状态机的方法：
将所有操作通过raft或者paxos的步骤；
以及通过一个配置中心，加主从复制（如cr），如gfs。（更为常见）；
前者在存储大规模数据时，快照(检查点）的传输会带来压力，后者的配置中心在状态上占用较小，大量数据由主从复制解决；

链式复制(CR)：
需要一个配置中心，读操作仅涉及一个server，即读tail。有简单的恢复方案。提供线性一致性；
先写入head，然后按照链式的顺序同步状态，即每个链表中的节点都有对应的磁盘；
等到tail完成写入并apply后，直接返回响应給客户端；

三种崩溃恢复方案：
● 当head宕机时，配置中心直接杀掉head即可，将head后的节点提升为head节点，客户端通过配置中心也刷新head节点的地址，可能有写操作仅存在于原head节点中，会丢失，是合法操作；
● 当中间节点宕机时，如同链表删除节点般，宕机的节点可能还没把写操作传递给下一个节点，所以宕机节点上一个节点需要检查缺失的传递操作；
● 当tail宕机时，配置中心直接杀掉tail即可，将tail前的节点变为tail节点，客户端同样可刷新（通过代理节点，这样可以保证不会访问到过期的节点）；

添加新节点：
简单的方法就是直接添加到tail，刚添加进链中，不会立刻成为tail，新tail先从原tail中拿取数据，可能gb或者tb级别的，耗时可能数分钟到小时；与此同时有一张update表，用于记录已经生效的数据，但是还未传到新tail的；当新tail完成复制后，请求原tail成为真实的tail，原tail响应（响应新tail，如果同时有客户端连接，也会同时响应客户端更换tail节点访问）后（此时的响应也可以看作第一次正常的链式复制的传递），添加新节点完成；

链式复制只是影响主备方案，并不影响配置中心；
相比于raft：
CR：
优点：
● 客户端的请求RPC会打到head或者tail节点上（拆分读写操作），一定程度上减轻压力；
● 只需向头节点发送一个写请求RPC即可；
● 读取操作仅涉及tail节点；
● 故障恢复的方法很简单；
缺点：
● 任一节点发生故障时，可视为链表断开（即使恢复很简单，但是服务因此还是会中断一会），所以必须立刻进行故障恢复；

CR的读请求优化：
将一个资源拆分到多条链中（分片）；
注意：此处的多条链中的节点可以是重复的，即假设有三个节点，123，312，231这样的三条链是合法的，且此时，每个节点在链中，都充当了一次head和tail，可以看作一种负载均衡（读吞吐量随尾节点的数量增多而线性增加）；
同时满足线性一致性和可扩展性（scale）；
水平扩展（scale out）或垂直扩展（scale up）
